# Browserside-Inferencing
ML models that run on your browsers

- Model file is taken from hugging face model hub [here](https://huggingface.co/martin-ha/toxic-comment-model)
- Model file is wasm file through onnx. The Wasm files can be directly run on browsers. 
- Since the model is pretty large it takes sometime for CDN to deliver it to the browser. Once that is done the inferncing is pretty fast in comparision to server side inferencing. 

![image](https://user-images.githubusercontent.com/60213893/224534002-1d835384-b507-4ae2-b292-893346ceaa84.png)

